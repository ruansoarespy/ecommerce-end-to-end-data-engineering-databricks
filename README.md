# ğŸ›’ E-commerce Data Engineering â€” Databricks Medallion Architecture  
## Projeto End-to-End usando Olist Dataset (PySpark + Delta Lake)

Este repositÃ³rio apresenta um pipeline completo de Engenharia de Dados utilizando a Arquitetura **Medallion (Bronze â†’ Silver â†’ Gold)** no **Databricks**, processando o dataset pÃºblico **Olist**.  
O projeto replica o fluxo real de um ambiente corporativo de dados: ingestÃ£o, processamento, qualidade, modelagem e disponibilizaÃ§Ã£o analÃ­tica.

---

## ğŸ“‚ Estrutura Geral do Projeto
.
â”œâ”€â”€ bronze/              # Dados crus do Olist em Delta
â”œâ”€â”€ silver/              # Dados tratados e integrados
â”œâ”€â”€ gold/                # Tabelas analÃ­ticas e mÃ©tricas
â”œâ”€â”€ notebooks/           # Notebooks organizados por camada
â”œâ”€â”€ sql/                 # Comandos Delta (OPTIMIZE, VACUUM, MERGE)
â”œâ”€â”€ docs/                # Diagramas, dicionÃ¡rio, documentaÃ§Ã£o
â””â”€â”€ README.md            # Este arquivo


/data
/bronze # CSVs brutos
/silver # tabelas Delta limpas
/gold # modelos analÃ­ticos

/docs
schema_reference.md
data_quality.md

README.md

yaml
Copiar cÃ³digo

---

## ğŸ§± Arquitetura Medallion

### ğŸ¥‰ Bronze â€” Raw Layer
- Recebe arquivos **CSV** diretamente do dataset Olist.  
- Nenhuma transformaÃ§Ã£o aplicada.  
- Apenas ingestÃ£o com `inferSchema` e `header=True`.  
- Armazenado como **Delta Lake** para permitir time travel e versionamento.

**Objetivo:** garantir que os dados brutos sejam preservados sem alteraÃ§Ã£o.

---

### ğŸ¥ˆ Silver â€” Refined Layer
- PadronizaÃ§Ã£o de colunas  
- ConversÃ£o de tipos  
- NormalizaÃ§Ã£o de datas  
- RemoÃ§Ã£o de duplicatas  
- CorreÃ§Ã£o de registros inconsistentes  
- Enriquecimento leve (ex: join cliente-endereÃ§o)

**Camada jÃ¡ concluÃ­da no projeto.**

---

### ğŸ¥‡ Gold â€” Business Layer  
Camada orientada ao negÃ³cio, pronta para BI e anÃ¡lises sofisticadas.

Tabelas principais:

#### **1. order_facts**
- Fato de pedidos com granularidade por pedido-item  
- Valores de receita, frete, totais e prazos

#### **2. customer_dim**
- DimensÃ£o de clientes com histÃ³rico agregÃ¡vel

#### **3. product_dim**
- DimensÃ£o de produtos, categorias traduzidas e medidas

#### **4. seller_dim**
- DimensÃ£o de vendedores

#### **5. rfm_customer_features**
- Recency  
- Frequency  
- Monetary  
Prontos para clustering ou LTV models.

---

## ğŸš€ Tecnologias
- **Databricks Community Edition**
- **PySpark**
- **Delta Lake**
- **ETL/ELT**
- **Business Analytics**
- **Power BI**

---

## ğŸ“Œ Fluxo do Pipeline

1. Upload dos CSVs no DBFS (`/ecommerce/bronze/2/â€¦`)
2. Leitura e armazenamento em Delta (Bronze)
3. Limpeza, conversÃµes e dedupe (Silver)
4. Modelagem dimensional e tabelas fato/dimensÃ£o (Gold)
5. Consumo no Power BI (via Databricks SQL Endpoint)

---

## ğŸ¯ Objetivos do Projeto

- Demonstrar domÃ­nio real em **Data Engineering**
- Criar um pipeline completo e reproduzÃ­vel
- Aplicar boas prÃ¡ticas (SCD, dedupe, padronizaÃ§Ã£o)
- Criar modelos analÃ­ticos sÃ³lidos para BI
- Mostrar senioridade em Databricks + Delta Lake

---

## ğŸ“Š Dashboard (Power BI)
O dashboard final inclui:

- Vendas por categoria  
- Faturamento por mÃªs  
- AnÃ¡lise de entregas (SLA, atrasos, lead time)  
- Mapa de geolocalizaÃ§Ã£o  
- Customer RFM  

---

## ğŸ“˜ Dataset
O projeto utiliza exclusivamente o dataset **Olist Brazilian E-Commerce**, composto por:

- Pedidos  
- Clientes  
- Itens  
- Pagamentos  
- Produtos  
- Vendedores  
- Categorias traduzidas  
- GeolocalizaÃ§Ã£o

---

## ğŸ“ PrÃ³ximas Etapas
- Implementar testes de qualidade (Great Expectations)  
- Adicionar monitoramento (Delta Live Tables)  
- Adicionar particionamento e ZORDER  
- Criar pipeline Airflow opcional  

---

Contato / Autor

**Ruan Ferreira Soares** â€” https://www.linkedin.com/in/ruan-soares123/
Engenharia de Dados â€¢ PySpark â€¢ Delta Lake â€¢ Databricks  

